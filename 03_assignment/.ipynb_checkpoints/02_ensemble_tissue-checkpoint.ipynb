{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ensemble-based models to predict tissue\n",
    "We are interested in predicting the tissue of origin using the latent spaces generated before.\n",
    "\n",
    "Using samples projected into the different latent spaces, use at least two ensemble approaches to predict their tissue of origin:\n",
    "One approach must use a learner/estimator that tends to overfit data.\n",
    "Another approach must use a weak learner that slightly outperforms a random estimator.\n",
    "Use a model evaluation strategy that allows you to play with different hyperparameters to select the best model, assess whether the models are good predictors across all tissues (take a look at the classification_reportLinks to an external site. function in sklearn) and whether they generalize well on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import adjusted_rand_score as ari\n",
    "from sklearn.metrics import adjusted_mutual_info_score as ami\n",
    "from sklearn.metrics import (\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    silhouette_score,\n",
    ")\n",
    "import umap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from 01_preprocessing jupyter notebook\n",
    "GTEx_data = pd.read_pickle(\"../GTEx_data_input.pkl\")\n",
    "GTEx_labels = pd.read_pickle(\"../GTEx_labels.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up PCA of GTEx data based on best parameters from 01_dimension_reduction\n",
    "# reduce GTEx data to a PCA of 40 dimensions, considered best\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca_fit = pca.fit(GTEx_data)\n",
    "pca_data = pca_fit.transform(GTEx_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up UMAP of GTEx data based on best parameters from 01_dimension_reduction\n",
    "# reduce data with UMAP, set seed because stochastic\n",
    "umap = umap.UMAP(random_state=42, n_components=200, min_dist=0.1)\n",
    "umap_fit = umap.fit(GTEx_data) \n",
    "umap_data = umap_fit.transform(GTEx_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up smaller datasets with colon and brain tissue info removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the full top ten tissue dataset to assess different parameters of the model. Then, we'll remove colon and brain tissue information from the dataset, train models using optimal hyperparameters, and then determine how well they work on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weak Learners: Boosting with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strong Learners: Bagging with Random Forest meta-estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of bagging is to reduce model variance by averaging many models trained on random subsets of the data.\n",
    "\n",
    "```\n",
    "for i in range(n_models):\n",
    "    # collect data samples and fit models\n",
    "    X_i, y_i = sample_with_replacement(X, y, n_samples)\n",
    "    model = Model().fit(X_i, y_i)\n",
    "    ensemble.append(model)\n",
    "\n",
    "# output average prediction at test time:\n",
    "y_test = ensemble.average_prediction(x_test)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging classifier: https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "\n",
    "When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting oob_score=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpbs7602_assignment03",
   "language": "python",
   "name": "cpbs7602_assignment03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
